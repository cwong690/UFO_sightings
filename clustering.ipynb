{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5/6/2017 05:00</td>\n",
       "      <td>Camp McGregor, NM</td>\n",
       "      <td>Light</td>\n",
       "      <td>10 minute</td>\n",
       "      <td>Light seen over mountain's east of Camp McGre...</td>\n",
       "      <td>Report appears to us to be consistent with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5/5/2017 11:30</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Disk</td>\n",
       "      <td>3 second</td>\n",
       "      <td>Flying saucer descends, possibly lands in Nor...</td>\n",
       "      <td>We would like to communicate with this witne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5/5/2017 03:00</td>\n",
       "      <td>El Mirage, AZ</td>\n",
       "      <td>Circle</td>\n",
       "      <td>30 second</td>\n",
       "      <td>While letting my dog out, a very bright white...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5/4/2017 21:27</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>Circle</td>\n",
       "      <td>15 second</td>\n",
       "      <td>Orange round sphere.  Orange glowing sphere f...</td>\n",
       "      <td>We have amended the time above, to reflect a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5/4/2017 18:30</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>Teardrop</td>\n",
       "      <td>5 minute</td>\n",
       "      <td>Flying corkscrews  Looking to th east at abou...</td>\n",
       "      <td>Source of the report elects to remain anonym...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  0                    1           2          3  \\\n",
       "0           0   5/6/2017 05:00     Camp McGregor, NM       Light   10 minute   \n",
       "1           1   5/5/2017 11:30            Austin, TX        Disk    3 second   \n",
       "2           2   5/5/2017 03:00         El Mirage, AZ      Circle   30 second   \n",
       "3           3   5/4/2017 21:27           Phoenix, AZ      Circle   15 second   \n",
       "4           4   5/4/2017 18:30           Phoenix, AZ    Teardrop    5 minute   \n",
       "\n",
       "                                                   4  \\\n",
       "0   Light seen over mountain's east of Camp McGre...   \n",
       "1   Flying saucer descends, possibly lands in Nor...   \n",
       "2   While letting my dog out, a very bright white...   \n",
       "3   Orange round sphere.  Orange glowing sphere f...   \n",
       "4   Flying corkscrews  Looking to th east at abou...   \n",
       "\n",
       "                                                   5  \n",
       "0    Report appears to us to be consistent with t...  \n",
       "1    We would like to communicate with this witne...  \n",
       "2                                                 []  \n",
       "3    We have amended the time above, to reflect a...  \n",
       "4    Source of the report elects to remain anonym...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/ufo.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Light seen over mountain's east of Camp McGre...\n",
       "1         Flying saucer descends, possibly lands in Nor...\n",
       "2         While letting my dog out, a very bright white...\n",
       "3         Orange round sphere.  Orange glowing sphere f...\n",
       "4         Flying corkscrews  Looking to th east at abou...\n",
       "                               ...                        \n",
       "11926     reddish orange triangular pattern of lights. ...\n",
       "11927     6 bright red glowing spheres.  Six red sphere...\n",
       "11928     In daylight, oval object traveled across sky ...\n",
       "11929     Triangular craft cary nc  On Sturdivant, saw ...\n",
       "11930     I've been watching videos on Youtube then, I ...\n",
       "Name: 4, Length: 11931, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pre = df.iloc[:, 5]\n",
    "text_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doc in enumerate(df.iloc[:, 5]):\n",
    "    #df.iloc[:, 5].str.replace('\\d+', '') # for digits\n",
    "    df.iloc[idx, 5] = doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 5] = df.iloc[:, 5].str.replace('\\d+', '') # for digits\n",
    "df.iloc[:, 5] = df.iloc[:, 5].str.replace(r'(\\b\\w{1,2}\\b)', '') # for words\n",
    "#df.iloc[:, 5] = df.iloc[:, 5].str.replace('[^\\w\\s]', '') # for punctuation \n",
    "df.iloc[:, 5] = df.iloc[:, 5].str.replace(r'[^\\w\\s]+', '')\n",
    "df.iloc[:, 5] = df.iloc[:, 5].str.lower()\n",
    "#df.iloc[:, 5] = df.iloc[:, 5].str.replace(string.punctuation, '') # for punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         light seen over mountain east  camp mcgregor ...\n",
       "1         flying saucer descends possibly lands  north ...\n",
       "2         while letting  dog out  very bright white cir...\n",
       "3         orange round sphere  orange glowing sphere fl...\n",
       "4         flying corkscrews  looking   east  about   sa...\n",
       "                               ...                        \n",
       "11926     reddish orange triangular pattern  lights   a...\n",
       "11927      bright red glowing spheres  six red spheres ...\n",
       "11928      daylight oval object traveled across sky fro...\n",
       "11929     triangular craft cary    sturdivant saw silen...\n",
       "11930      been watching videos  youtube then  look out...\n",
       "Name: 4, Length: 11931, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.iloc[:, 5]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [pipelinize source](https://evisionindia.wordpress.com/2020/03/06/setting-up-text-preprocessing-pipeline-using-scikit-learn-and-spacy-learn-how-to-tokenize-lemmatize-remove-stop-words-and-punctuation-with-sklearn-pipelines/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from string import *\n",
    "\n",
    "# class PorterTokenizer:\n",
    "#      def __init__(self):\n",
    "            \n",
    "#             self.ps = PorterStemmer()\n",
    "#             self.lower      = lower\n",
    "#             self.strip      = strip\n",
    "#             self.stopwords  = stopwords or set(sw.words('english'))\n",
    "#             self.punct      = punct or set(string.punctuation)\n",
    "#             self.lemmatizer = WordNetLemmatizer()\n",
    "#      def __call__(self, doc):\n",
    "#          return [self.ps.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize          \n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# class LemmaTokenizer(object):\n",
    "#     def __init__(self):\n",
    "#         self.wnl = WordNetLemmatizer()\n",
    "#     def __call__(self, articles):\n",
    "#         return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "# tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "#                                 strip_accents = 'unicode', # works \n",
    "#                                 stop_words = 'english', # works\n",
    "#                                 lowercase = True, # works\n",
    "#                                 max_df = 0.5, # works\n",
    "#                                 min_df = 10) # works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "n_features=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feli/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11931, 10000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(stop_words='english', max_features=n_features,\n",
    "                       ngram_range=(1,2),\n",
    "                       tokenizer=LemmaTokenizer(),\n",
    "                       strip_accents = 'unicode', # works \n",
    "                       lowercase = True, # works\n",
    "                       max_df = 0.5, # works\n",
    "                       min_df = 10 # works\n",
    "                       #, tokenizer=PorterTokenizer()\n",
    "                      )\n",
    "words = vect.fit_transform(text)\n",
    "V = words.toarray()\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.9345643846546"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components)\n",
    "nmf.fit(V)\n",
    "W = nmf.transform(V)\n",
    "H = nmf.components_\n",
    "\n",
    "nmf.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: like, bright, saw, craft, just, looked, star, red, moving, white\n",
      "2: provides, information, anonymous, elect remain, elect, contact information, remain, anonymous provides, remain totally, totally anonymous\n",
      "3: object, object wa, appeared, shaped, flying, approximately, moving, east, observed, west\n"
     ]
    }
   ],
   "source": [
    "index_val = np.argsort(H)[:, -1:-11:-1]\n",
    "\n",
    "for i, lat_feat in enumerate(index_val):\n",
    "    print('%d: %s'%(i+1,', '.join([feature_names[n] for n in lat_feat])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.221477s\n",
      "Explained variance of the SVD step: 2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(words)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "       n_clusters=10, n_init=1, n_jobs=None, precompute_distances='auto',\n",
      "       random_state=None, tol=0.0001, verbose=False)\n",
      "done in 0.128s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=10, init='k-means++', max_iter=100, n_init=1,\n",
    "            verbose=False)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "# print(\"Adjusted Rand-Index: %.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 1: like saw bright craft looked\n",
      "Cluster 2: provides information anonymous elect remain elect\n",
      "Cluster 3: object bright like saw moving\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "#     else:\n",
    "#         order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vect.get_feature_names()\n",
    "for i in range(1, n_components +1):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
