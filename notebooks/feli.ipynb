{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # to work with json file format\n",
    "from bs4 import BeautifulSoup # to parse html\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data cleaning\n",
    "# # no longer needed with the csv\n",
    "# reports = []\n",
    "# with open('data/ufodata.json') as f:\n",
    "#     for i, k in enumerate(f):\n",
    "# #         if (i % 100 == 0):\n",
    "# #             print(i)\n",
    "#         try:\n",
    "#             reports.append(json.loads(k))\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "# # finds each key in json file for that record\n",
    "# print(reports[0].keys(), '\\n')\n",
    "\n",
    "# # prints each key and value (using f strings)\n",
    "# for k, v in reports[0].items():\n",
    "#     print(f'{k}: {v} \\n')\n",
    "\n",
    "# def strip_html(item):\n",
    "#     \"\"\" Item represent obervation (list of 3 features)\"\"\"\n",
    "#     cleaned = []\n",
    "#     for i in item:\n",
    "#         i = str(i)\n",
    "#         str1 = i.replace('<font color=\\\"#000000\\\" face=\\\"Calibri\\\" style=\\\"FONT-SIZE:11pt\\\">', '')\n",
    "#         str2 = str1.replace('</font>', '').replace('<br/>',' ')\n",
    "#         cleaned.append(str2)        \n",
    "#     return cleaned\n",
    "\n",
    "# def extract_occured(item):\n",
    "#     pass\n",
    "\n",
    "# def extract_location(item):\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "# clean_sightings = []\n",
    "# for obs in soups:\n",
    "#     clean_sightings.append(strip_html(obs))   \n",
    "\n",
    "# sample = clean_sightings[:5]\n",
    "# df = pd.DataFrame(sample, columns=['report', 'occured', 'text'])\n",
    "\n",
    "# sample[0]\n",
    "\n",
    "# df\n",
    "\n",
    "# df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Here with importing cleaned data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Location</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Text</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/6/2017 05:00</td>\n",
       "      <td>Camp McGregor, NM</td>\n",
       "      <td>Light</td>\n",
       "      <td>10 minute</td>\n",
       "      <td>Light seen over mountain's east of Camp McGre...</td>\n",
       "      <td>Report appears to us to be consistent with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/5/2017 11:30</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Disk</td>\n",
       "      <td>3 second</td>\n",
       "      <td>Flying saucer descends, possibly lands in Nor...</td>\n",
       "      <td>We would like to communicate with this witne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/4/2017 21:27</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>Circle</td>\n",
       "      <td>15 second</td>\n",
       "      <td>Orange round sphere.  Orange glowing sphere f...</td>\n",
       "      <td>We have amended the time above, to reflect a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/4/2017 18:30</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>Teardrop</td>\n",
       "      <td>5 minute</td>\n",
       "      <td>Flying corkscrews  Looking to th east at abou...</td>\n",
       "      <td>Source of the report elects to remain anonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/4/2017 04:50</td>\n",
       "      <td>Taft, CA</td>\n",
       "      <td>Changing</td>\n",
       "      <td>20 second</td>\n",
       "      <td>I'm a truck driver and I've seen the reddish/...</td>\n",
       "      <td>Witness indicates \"Taft, Indiana\" in origina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Datetime             Location       Shape   Duration  \\\n",
       "0   5/6/2017 05:00     Camp McGregor, NM       Light   10 minute   \n",
       "1   5/5/2017 11:30            Austin, TX        Disk    3 second   \n",
       "2   5/4/2017 21:27           Phoenix, AZ      Circle   15 second   \n",
       "3   5/4/2017 18:30           Phoenix, AZ    Teardrop    5 minute   \n",
       "4   5/4/2017 04:50              Taft, CA    Changing   20 second   \n",
       "\n",
       "                                                Text  \\\n",
       "0   Light seen over mountain's east of Camp McGre...   \n",
       "1   Flying saucer descends, possibly lands in Nor...   \n",
       "2   Orange round sphere.  Orange glowing sphere f...   \n",
       "3   Flying corkscrews  Looking to th east at abou...   \n",
       "4   I'm a truck driver and I've seen the reddish/...   \n",
       "\n",
       "                                               Notes  \n",
       "0    Report appears to us to be consistent with t...  \n",
       "1    We would like to communicate with this witne...  \n",
       "2    We have amended the time above, to reflect a...  \n",
       "3    Source of the report elects to remain anonymous  \n",
       "4    Witness indicates \"Taft, Indiana\" in origina...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2736 rows of dataset\n",
    "df = pd.read_csv('../data/ufo.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Light seen over mountain's east of Camp McGregor.  It hovered in one spot.  It looked like a helicopter light at first.  But it was way to bright.  Then it went to a higher height.  Stayed there for a while.  Then disappeared.   ((NUFORC Note:  Report appears to us to be consistent with the sighting of Venus.  PD)) \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cindywong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input string outputs bag of words\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_bow_from_raw_text(text_as_string):\n",
    "#     \"\"\"Extracts bag-of-words from a raw text string.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     text (str): a text document given as a string\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     list : the list of the tokens extracted and filtered from the text\n",
    "#     \"\"\"\n",
    "#     if (text_as_string == None):\n",
    "#         return []\n",
    "\n",
    "#     if (len(text_as_string) < 1):\n",
    "#         return []\n",
    "\n",
    "#     nfkd_form = unicodedata.normalize('NFKD', text_as_string)\n",
    "#     text_input = str(nfkd_form.encode('ASCII', 'ignore'))\n",
    "\n",
    "#     sent_tokens = sent_tokenize(text_input)\n",
    "\n",
    "#     tokens = list(map(word_tokenize, sent_tokens))\n",
    "\n",
    "#     sent_tags = list(map(pos_tag, tokens))\n",
    "\n",
    "#     grammar = r\"\"\"\n",
    "#         SENT: {<(J|N).*>}                # chunk sequences of proper nouns\n",
    "#     \"\"\"\n",
    "\n",
    "#     cp = RegexpParser(grammar)\n",
    "#     ret_tokens = list()\n",
    "#     stemmer_snowball = SnowballStemmer('english')\n",
    "\n",
    "# #     for sent in sent_tags:\n",
    "# #         tree = cp.parse(sent)\n",
    "# #         for subtree in tree.subtrees():\n",
    "# #             if subtree.label() == 'SENT':\n",
    "# #                 t_tokenlist = [tpos[0].lower() for tpos in subtree.leaves()]\n",
    "# #                 t_tokens_stemsnowball = list(map(stemmer_snowball.stem, t_tokenlist))\n",
    "# #                 #t_token = \"-\".join(t_tokens_stemsnowball)\n",
    "# #                 #ret_tokens.append(t_token)\n",
    "# #                 ret_tokens.extend(t_tokens_stemsnowball)\n",
    "# #             #if subtree.label() == 'V2V': print(subtree)\n",
    "# #     #tokens_lower = [map(string.lower, sent) for sent in tokens]\n",
    "\n",
    "# #     wordnet = WordNetLemmatizer()\n",
    "# #     docs_wordnet = [[wordnet.lemmatize(word) for word in words]\n",
    "# #                     for words in tokens]\n",
    "    \n",
    "#     return(ret_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'light\",\n",
       " 'sky',\n",
       " 'stationari',\n",
       " 'airplan',\n",
       " 'known',\n",
       " 'star',\n",
       " 'bright',\n",
       " 'nuforc',\n",
       " 'note',\n",
       " 'venus',\n",
       " 'sourc',\n",
       " 'report',\n",
       " 'anonym',\n",
       " 'contact',\n",
       " 'inform',\n",
       " 'citi',\n",
       " 'mojav',\n",
       " 'bc',\n",
       " 'canada',\n",
       " 'pd']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(extract_bow_from_raw_text(df['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bags = []\n",
    "for i in range(df.shape[0]):\n",
    "    bag = extract_bow_from_raw_text(df['text'][i])\n",
    "    word_bags.append(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bags_series = pd.Series(word_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_bags'] = word_bags_series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'light\",\n",
       " 'sky',\n",
       " 'stationari',\n",
       " 'airplan',\n",
       " 'known',\n",
       " 'star',\n",
       " 'bright',\n",
       " 'nuforc',\n",
       " 'note',\n",
       " 'venus',\n",
       " 'sourc',\n",
       " 'report',\n",
       " 'anonym',\n",
       " 'contact',\n",
       " 'inform',\n",
       " 'citi',\n",
       " 'mojav',\n",
       " 'bc',\n",
       " 'canada',\n",
       " 'pd']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_bags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF Topic modeling\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=6)\n",
    "nmf.fit(V)\n",
    "W = nmf.transform(V)\n",
    "H = nmf.components_\n",
    "\n",
    "nmf.reconstruction_err_\n",
    "\n",
    "\n",
    "\n",
    "# groups everything in 10 groups\n",
    "\n",
    "index_val = np.argsort(H)[:, -1:-11:-1]\n",
    "\n",
    "for i, lat_feat in enumerate(index_val):\n",
    "    print('%d: %s'%(i+1,', '.join([feature_names[n] for n in lat_feat])))\n",
    "\n",
    "    \n",
    "# labels the topics\n",
    "labels = [?]\n",
    "\n",
    "for topic, lat_feat in zip(topics, index_val):\n",
    "    print('%s: %s'%(topic,', '.join([feature_names[n] for n in lat_feat])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "count_vect.fit(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CountVectorizer.get_feature_names of CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/ufo.csv')\n",
    "text = df_test.iloc[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 6\n",
    "n_features=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 45.690279s\n",
      "n_samples: 3486, n_features: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ##hashing no idf\n",
    "# vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "#                                stop_words='english',\n",
    "#                                alternate_sign=False, norm='l2')\n",
    "\n",
    "##hashing w/ idf\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=n_features,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(max_df=0.5, max_features=n_features,\n",
    "#                                  min_df=2, stop_words='english',\n",
    "#                                  use_idf=True)\n",
    "# X = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.010445s\n",
      "Explained variance of the SVD step: 70%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = MiniBatchKMeans(n_clusters=5, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.035s\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-0eda522e0b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Homogeneity: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhomogeneity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completeness: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompleteness_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"V-measure: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_measure_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# t0 = time()\n",
    "# km.fit(X)\n",
    "# print(\"done in %0.3fs\" % (time() - t0))\n",
    "# print()\n",
    "\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "# print(\"Adjusted Rand-Index: %.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MiniBatchKMeans' object has no attribute 'cluster_centers_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-31be8cb684cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top terms per cluster:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moriginal_space_centroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0morder_centroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_space_centroids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         order_centroids = km.cluster_centers_.argsort()[:, ::-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MiniBatchKMeans' object has no attribute 'cluster_centers_'"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "#     else:\n",
    "#         order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(n_features):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
